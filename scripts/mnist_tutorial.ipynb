{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# `torch_blue` basic tutorial\n",
    "\n",
    "This tutorial will run you through the basic process of implementing a Bayesian Neural\n",
    "Network (BNN). It assumes that you have basic familiarity with implementing neural\n",
    "networks in PyTorch and mostly highlight the (typically small) differences in usage.\n",
    "Specifically, this tutorial demonstrates how to implement the same model showcased in the\n",
    "[PyTorch Quickstart tutorial](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "as a BNN, while pointing to more advanced features to read up on in the documentation.\n",
    "\n",
    "This tutorial also provides several block with optional, more detailed information, like\n",
    "the one below. They aim to provide you with a basic understanding of why certain things\n",
    "are required or how they work. If you just wish to get a trainable model feel free to\n",
    "skip them and maybe come back later with more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "> **Variational Inference**\n",
    ">\n",
    "> `torch_blue.vi` implements BNNs via a variational inference method often know as\n",
    "> Bayes-by-backprop. Fundamentally, it describes the weights as distributions by\n",
    "> representing the distribution of each weight with the parameters of an assumed\n",
    "> distribution type. More simply put and for the most common usage, each weight is\n",
    "> described by the mean and standard deviation (std) of a Gaussian distribution. When\n",
    "> making a forward pass a sample from each weight distribution, which are used as\n",
    "> weights for that single forward pass. To obtain an output distribution this forward\n",
    "> pass is run several times with different samples. Training the model means training\n",
    "> all these means and stds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "This tutorial uses type hinting, so a couple extra imports are needed. Finally, we also\n",
    "import `torch_blue.vi`, which takes a very similar role to `torch.nn` but for BNN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch_blue import vi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Setting up datasets is completely unchanged from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Creating a dataloader also remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for x, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Creating Models\n",
    "\n",
    "To set up a BNN you will have to replace non-Bayesian PyTorch layers with Bayesian\n",
    "`torch_blue` layers. This affects only layers that have weights. Currently, `torch_blue`\n",
    "supports linear layers (`VILinear`), convolution layers (`VIConv1d`, `VIConv2d`,\n",
    "`VIConv3d`) and Transformer layers (`VITransformer`, `VIMultiheadAttention`). These can\n",
    "all be imported from `torch_blue.vi`. As you might have noticed it is as simple as\n",
    "importing from `torch_blue.vi` instead of `torch.nn` and prepending `VI` to the layer\n",
    "name.\n",
    "\n",
    "Note that while there a many more layer types, most of them do not need to be made\n",
    "Bayesian, either because they do not have weights (like activation functions and\n",
    "pooling layers) or because it does not necessarily make sense to make the Bayesian due\n",
    "to their purpose (mostly normalization layers like layer norms). `torch_blue` layers\n",
    "seamlessly combine with PyTorch layers. However, there is one important thing to\n",
    "consider:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> When implementing a Bayesian model with torch_blue the outermost\n",
    "layer must always inherit from vi.VIModule instead of nn.Module. This may only be a\n",
    "wrapper fowards the inputs and outputs, but it is essential for functionality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): VILinear()\n",
      "    (1): ReLU()\n",
      "    (2): VILinear()\n",
      "    (3): ReLU()\n",
      "    (4): VILinear()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(vi.VIModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            vi.VILinear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            vi.VILinear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            vi.VILinear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_: Tensor) -> Tensor:\n",
    "        x_ = self.flatten(x_)\n",
    "        logits = self.linear_relu_stack(x_)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**TLDR:** Modules should be subclasses of `vi.VIModule` instead of `nn.Module` and\n",
    "layers with weights need to be replaced with Bayesian layers, which can be found in\n",
    "`torch_blue.vi` and simply add `VI` in front of their usual name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "> **Why do I need to inherit from `vi.VIModule`?**\n",
    ">\n",
    "> As mentioned in the explanation on variational inference several forward passes need\n",
    "> to be run to obtain an output distribution. `torch_blue` automates this process by\n",
    "> duplicating the inputs in an additional sample dimension and vectorizing the forward\n",
    "> pass over this dimension making the process relatively efficient especially when used\n",
    "> with GPUs. However, this process needs to be preformed only once and not for each\n",
    "> layer. Therefore, `torch_blue` automatically detects the outermost `VIModule` and\n",
    "> makes it perform this task. Additionally, loss calculation requires knowledge on the\n",
    "> probability of the actual weight samples. The outermost module also gathers and\n",
    "> integrates this information into the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Optimizing the Model Parameters\n",
    "\n",
    "A Bayesian model needs to be trained on a Bayesian loss that also optimizes for the\n",
    "correct uncertainties. In `torch_blue` this is the `KullbackLeiblerLoss`, which is\n",
    "minimal if the weight distributions are identical the ideal weight distribution implied\n",
    "by the data.\n",
    "\n",
    "It works for any task, but requires a task specific predictive distribution. This\n",
    "distribution specifies your expectation on the distribution of your outputs. For\n",
    "regression task this will typically be `MeanFieldNormal` (from\n",
    "`torch_blue.vi.distributions`), which makes the loss behave similar to `MSELoss` (from\n",
    "`torch.nn`). Since the task here is classification we will use a `Categorical`\n",
    "distribution (again from `torch_blue.vi.distributions`), which will make the loss\n",
    "behave similar to `CrossEntropyLoss` (from `torch.nn`). Finally, the loss requires the\n",
    "total number of data samples (**not** batches!).\n",
    "\n",
    "The optimizer can be set as usual.\n",
    "\n",
    "> **Advanced note:** If you are optimizing for computational speed you might want to\n",
    "> have a look at `vi.AnalyticalKullbackLeiblerLoss`, which cannot be used in all cases,\n",
    "> but may reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_blue.vi.distributions import Categorical\n",
    "\n",
    "predictive_distribution = Categorical()\n",
    "loss_fn = vi.KullbackLeiblerLoss(\n",
    "    predictive_distribution, dataset_size=len(training_data)\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "> **Why Adam and not SGD?**\n",
    ">\n",
    "> Variational inference can be somewhat prone to exploding gradient effects specifically\n",
    "> in early epochs. This can be fixed by carefully adjusting hyperparameters or\n",
    "> introducing learning rate warm-up. Adam also avoids this issue likely due to\n",
    "> gradient clipping and is the simplest approach for the purposes of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Besides the changed model and loss, the training loop is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataloader: DataLoader,\n",
    "    model: vi.VIModule,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "When checking the shape of your output, you will notice an additional dimension. This is\n",
    "a sample dimension that `torch_blue` generates automatically. To properly represent the\n",
    "output distribution a BNN needs to be run several times on the same sample and the\n",
    "results are aggregated. `torch_blue` automates this process in the predictive\n",
    "distribution. Inside your model you can ignore this dimension due to vectorization.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> The output of torch_blue models is not a normal Tensor, but a VIReturn\n",
    "object. This behaves the same as a Tensor, but performing any operations on it before\n",
    "passing it to the loss may break loss computation. Therefore, try to integrate all\n",
    "operation into your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "> **A Note on samples**\n",
    ">\n",
    "> You may want to change the number of samples per forward pass for better sampling or\n",
    "> lower computational load. This can be done by passing the keyword argument `samples`\n",
    "> (default=10) to the forward call of the model, which specifies the number of samples\n",
    "> to use. It is automatically caught by `torch_blue` in the vectorization process.\n",
    "> We have found that 10 tends to strike a good balance between accuracy and compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "> **What is `VIReturn`?**\n",
    ">\n",
    "> `VIReturn` is a thin wrapper around `torch.Tensor`. All it does is add the attribute\n",
    "> `log_probs`, which is used to store the weight probability information mentioned in\n",
    "> the block on `VIModule` and pass it to the loss. Since PyTorch is not aware of this\n",
    "> any operation that creates a new Tensor works, but will lose the `log_probs`\n",
    "> attribute, which is required for loss computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The only change in the test loop is, that the predictive distribution is used to\n",
    "aggregate the sample dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader: DataLoader, model: vi.VIModule, loss_fn: Callable) -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            samples = model(x)\n",
    "            test_loss += loss_fn(samples, y).item()\n",
    "\n",
    "            pred = predictive_distribution.predictive_parameters_from_samples(\n",
    "                samples\n",
    "            )\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Time to bring it home and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1979140.625000  [   64/60000]\n",
      "loss: 1849943.125000  [ 6464/60000]\n",
      "loss: 1786325.250000  [12864/60000]\n",
      "loss: 1750446.375000  [19264/60000]\n",
      "loss: 1701688.750000  [25664/60000]\n",
      "loss: 1652999.125000  [32064/60000]\n",
      "loss: 1606158.250000  [38464/60000]\n",
      "loss: 1572845.000000  [44864/60000]\n",
      "loss: 1523578.000000  [51264/60000]\n",
      "loss: 1481647.375000  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 1457582.563694 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1448494.500000  [   64/60000]\n",
      "loss: 1409645.875000  [ 6464/60000]\n",
      "loss: 1359520.500000  [12864/60000]\n",
      "loss: 1324419.750000  [19264/60000]\n",
      "loss: 1279386.500000  [25664/60000]\n",
      "loss: 1240244.875000  [32064/60000]\n",
      "loss: 1194377.250000  [38464/60000]\n",
      "loss: 1163121.750000  [44864/60000]\n",
      "loss: 1127201.250000  [51264/60000]\n",
      "loss: 1083609.875000  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 1068581.152070 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1059136.000000  [   64/60000]\n",
      "loss: 1026232.562500  [ 6464/60000]\n",
      "loss: 988634.812500  [12864/60000]\n",
      "loss: 960740.187500  [19264/60000]\n",
      "loss: 930759.187500  [25664/60000]\n",
      "loss: 895041.687500  [32064/60000]\n",
      "loss: 867782.937500  [38464/60000]\n",
      "loss: 850785.625000  [44864/60000]\n",
      "loss: 818872.125000  [51264/60000]\n",
      "loss: 801681.625000  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 780968.631768 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 769400.312500  [   64/60000]\n",
      "loss: 753410.687500  [ 6464/60000]\n",
      "loss: 726418.375000  [12864/60000]\n",
      "loss: 711143.875000  [19264/60000]\n",
      "loss: 693142.187500  [25664/60000]\n",
      "loss: 674597.937500  [32064/60000]\n",
      "loss: 646707.625000  [38464/60000]\n",
      "loss: 646525.875000  [44864/60000]\n",
      "loss: 620407.250000  [51264/60000]\n",
      "loss: 611824.437500  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 600053.687898 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 589260.000000  [   64/60000]\n",
      "loss: 579646.125000  [ 6464/60000]\n",
      "loss: 563087.312500  [12864/60000]\n",
      "loss: 554328.250000  [19264/60000]\n",
      "loss: 546699.500000  [25664/60000]\n",
      "loss: 531331.312500  [32064/60000]\n",
      "loss: 516979.500000  [38464/60000]\n",
      "loss: 518027.562500  [44864/60000]\n",
      "loss: 500177.281250  [51264/60000]\n",
      "loss: 485142.250000  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 486797.256568 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
